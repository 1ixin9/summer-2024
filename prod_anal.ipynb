{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling the gpt\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 152\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_mk(ar1, ar2, ar3, ar4) \u001b[38;5;66;03m# finally we make the df w the arrays we built\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# creating the df\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcall_marketinGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# styling df\u001b[39;00m\n\u001b[1;32m    155\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[23], line 124\u001b[0m, in \u001b[0;36mcall_marketinGPT\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# prompt refers to the template ur using to prompt engineer -> this is given to llm\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# llm then takes the text input and generates a response\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# StrOutputParser is an output parser (DUH but also an output parser takes raw output and turns it into a structured format)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# ^ this says the output of one component should be used as the input of the next component\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# ^ so the prompt's output is the llm's input, the llm's output is the parser's input\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;66;03m# we using a try-except bc who knows if the GPT will output something that is always understandable\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     ans \u001b[38;5;241m=\u001b[39m \u001b[43mmarketinGPT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprod_des\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# marketinGPT (brilliant name) is the name of the pipeline\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# so when we call it, we r getting an instance of it\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# .invoke(input val) is a method that tells the model to provide a response based on the input val\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# \"prod\" is the input variable in the prompt, prod_des is the value in the for-each loop\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# this lets prod_des be passed in as the input of the prompt\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     parsed_response \u001b[38;5;241m=\u001b[39m parse_response(ans) \u001b[38;5;66;03m# here we use parse_response to parse the response (DUH)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/langchain_core/runnables/base.py:2495\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2494\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2495\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2496\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    167\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    169\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    180\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    460\u001b[0m ]\n\u001b[1;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 446\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/langchain_community/chat_models/baidu_qianfan_endpoint.py:299\u001b[0m, in \u001b[0;36mQianfanChatEndpoint._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m chat_generation_info: Dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages, stop, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    300\u001b[0m     chat_generation_info \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    301\u001b[0m         chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mgeneration_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m chat_generation_info\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m     completion \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/langchain_community/chat_models/baidu_qianfan_endpoint.py:390\u001b[0m, in \u001b[0;36mQianfanChatEndpoint._stream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[1;32m    389\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mdo(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res:\n\u001b[1;32m    392\u001b[0m         msg \u001b[38;5;241m=\u001b[39m _convert_dict_to_message(res)\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/qianfan/resources/requestor/openapi_requestor.py:333\u001b[0m, in \u001b[0;36mQfAPIRequestor._compensate_token_usage_stream\u001b[0;34m(self, resp, token_count)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp, Iterator):\n\u001b[1;32m    332\u001b[0m     token_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m resp:\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Ratelimit-Limit-Tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mheaders:\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_limiter\u001b[38;5;241m.\u001b[39mreset_once(\n\u001b[1;32m    336\u001b[0m                 \u001b[38;5;28mint\u001b[39m(res\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Ratelimit-Limit-Tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    337\u001b[0m             )\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/qianfan/resources/requestor/base.py:225\u001b[0m, in \u001b[0;36m_stream_latency.<locals>.wrapper.<locals>.iter\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m is_first_block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    223\u001b[0m sse_block_receive_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m resp:\n\u001b[1;32m    226\u001b[0m     r\u001b[38;5;241m.\u001b[39mstatistic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_latency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    227\u001b[0m         (time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m sse_block_receive_time)\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_first_block\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatistic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_token_latency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    230\u001b[0m     )\n\u001b[1;32m    231\u001b[0m     is_first_block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/qianfan/resources/requestor/openapi_requestor.py:112\u001b[0m, in \u001b[0;36mQfAPIRequestor._request_stream.<locals>.iter\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m         body, resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/qianfan/resources/http_client.py:93\u001b[0m, in \u001b[0;36mHTTPClient.request_stream\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# yield response first for checking meta info like status code\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# without waiting for the whole response\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(), resp\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m line, resp\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/requests/models.py:869\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    867\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[1;32m    870\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[1;32m    871\u001b[0m ):\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/urllib3/response.py:1040\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/urllib3/response.py:1184\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/urllib3/response.py:1108\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate # for creating the template we feed to llm\n",
    "from langchain_community.chat_models import QianfanChatEndpoint # for getting the actual GPT llm\n",
    "from langchain_core.output_parsers import StrOutputParser # for converting llm output to something Python understands\n",
    "import pandas as pd # for making dfs\n",
    "from IPython.display import display # for displaying the df with styling\n",
    "import time\n",
    "\n",
    "# key\n",
    "qianfan_ak = \"DAEEqjuvglLTgQMCXqRvqfUj\"\n",
    "qianfan_sk = \"s0AJ849GNB6440lwLWDvGuNEJNrgrbQ3\"\n",
    "\n",
    "# model\n",
    "llm = QianfanChatEndpoint(model=\"ERNIE-4.0-8K\", streaming=True, qianfan_ak=qianfan_ak, qianfan_sk=qianfan_sk, penalty_score=1)\n",
    "\n",
    "def df_mk(ar1, ar2, ar3, ar4):\n",
    "    \n",
    "    t1 = time.time()\n",
    "        \n",
    "    df = pd.DataFrame({ # this is the syntax for making a df which is basically a table in pandas\n",
    "        \"产品描述\": ar1, # the quotes has the title of the column\n",
    "        \"产品卖点\": ar2, # u can either make an array like [val, val] urself\n",
    "        \"最佳营销卖点\": ar3, # or u can use an array that u alr made\n",
    "        \"目标受众\": ar4 # and put it into the df like that\n",
    "    })\n",
    "    \n",
    "    t2 = time.time()\n",
    "    t = round(t2-t1)\n",
    "        \n",
    "    print(f\"\\nmaking df took {t} seconds\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "def parse_response(llm_output): # this parses the output param that is generated by the llm to find the info we need for the df\n",
    "    \n",
    "    # split the output into lines\n",
    "    lines = llm_output.split('\\n') # split function is a python func that turns a string into a list\n",
    "    # ^ where each newline from the og where the items r separated\n",
    "    \n",
    "    # initialize placeholders\n",
    "    product_description = 'missing description' # this is for debugging in case we find something isn't generated properly\n",
    "    selling_points = 'missing description' # then we know that whatever has \"missing description\" didn't have that part\n",
    "    best_marketing_point = 'missing description'\n",
    "    target_audience = 'missing description'\n",
    "        \n",
    "    # iterate over lines and find the relevant sections\n",
    "    for line in lines: # for each loop that searches each line in the output for the content we need\n",
    "        if line.startswith(\"1. **产品描述**：\"): # so if the line starts with [this] then we store this line into the corresponding var\n",
    "            product_description = line[len(\"1. **产品描述**：\"):].strip() # but we strip the title for cleanliness\n",
    "        elif line.startswith(\"2. **产品卖点**：\"):\n",
    "            selling_points = line[len(\"2. **产品卖点**：\"):].strip()\n",
    "        elif line.startswith(\"3. **最佳营销卖点**：\"):\n",
    "            best_marketing_point = line[len(\"3. **最佳营销卖点**：\"):].strip()\n",
    "        elif line.startswith(\"4. **目标受众**：\"):\n",
    "            target_audience = line[len(\"4. **目标受众**：\"):].strip()\n",
    "    \n",
    "    return product_description, selling_points, best_marketing_point, target_audience # and we just return the variables\n",
    "\n",
    "\n",
    "def call_marketinGPT():\n",
    "    \n",
    "    print(\"calling the gpt\\n\")\n",
    "\n",
    "    file_path = \"products.txt\" # this is the file u wanna open\n",
    "    # if it was in a diff path, then u would hv to do ../folder/folder/file.txt instead\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file: # 'r' means read (DUH), utf-8 encoding is standard\n",
    "        prod_descr = file.readlines() # \"with\" makes sure it's closed at the end\n",
    "    # \"as file\" basically sets what's opened into the variable \"file\"\n",
    "    # prod_descr is a list type var that stores everything that is read from the file\n",
    "    # readlines reads all the individual lines (broken apart by \\n) into the list\n",
    "    \n",
    "    prod_descr = [desc.strip() for desc in prod_descr] # prod_descr prior to this would be like [\"prod_des1\\n\", \"prod_des2\\n\"]\n",
    "    # desc is each individual line stored in prod_descr, using desc.strip for each desc gets rid of \\n at the end of each one\n",
    "    # now, prod_descr is like [\"prod_des1\", \"prod_des2\"]\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        # template is the prompt that ur using to prompt engineer the GPT\n",
    "        template=\"\"\"作为一名零售顾问助手，你的任务是帮助用户分析他们的产品描述，\n",
    "        并提供该产品的卖点、最佳营销卖点、目标受众以及针对目标受众的营销策略。\n",
    "        请根据以下格式进行回复，并且仅根据用户提供的信息进行分析和回答：\\n\\n\n",
    "            1. **产品描述**：用户提供的产品详细信息。\\n\n",
    "            2. **产品卖点**：根据产品描述，提炼出吸引潜在消费者的关键特点。\\n\n",
    "            3. **最佳营销卖点**：从产品卖点中选择最具市场潜力的特点，并解释为何这个卖点最有吸引力。\\n\n",
    "            4. **目标受众**：根据产品卖点，确定最适合的消费群体。\\n\\n\n",
    "            \n",
    "        以下是一个示例对话：\\n\n",
    "        \n",
    "        用户：我们有一款新型的可折叠电动自行车，重量轻，电池续航长，适合城市通勤。\\n\\n\n",
    "        系统：\\n\n",
    "            1. **产品描述**：新型可折叠电动自行车，重量轻，电池续航长，适合城市通勤。\\n\n",
    "            2. **产品卖点**：轻便设计、长续航电池、便捷的城市通勤工具。\\n\n",
    "            3. **最佳营销卖点**：长续航电池，因为城市通勤用户对续航时间有较高需求，能够减少充电频率。\\n\n",
    "            4. **目标受众**：城市白领、大学生、注重环保和便捷出行的用户。\\n\\n\n",
    "            \n",
    "        请提供您的产品描述：\\n\n",
    "        \n",
    "        {prod}\\n\\n\n",
    "\n",
    "        1. **产品描述**：用户提供的产品描述\\n\n",
    "        2. **产品卖点**：提炼出的产品卖点\\n\n",
    "        3. **最佳营销卖点**：选择的最佳营销卖点及其原因\\n\n",
    "        4. **目标受众**：确定的目标消费群体\\n\"\"\",\n",
    "        \n",
    "        input_variables=[\"prod\"] # here ur telling the gpt that the input variables it uses will be\n",
    "        # used where {prod} is used in the template\n",
    "    )\n",
    "    \n",
    "    ar1, ar2, ar3, ar4 = [], [], [], [] # here ur declaring the arrays that the df will be made w\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for prod_des in prod_descr: # this is a for-each loop which ensures that each val in the list is used\n",
    "        \n",
    "        t1 = time.time()\n",
    "                \n",
    "        marketinGPT = prompt | llm | StrOutputParser() # this is the setup for the processing pipeline\n",
    "        # prompt refers to the template ur using to prompt engineer -> this is given to llm\n",
    "        # llm then takes the text input and generates a response\n",
    "        # StrOutputParser is an output parser (DUH but also an output parser takes raw output and turns it into a structured format)\n",
    "        # ^ this turns the llm's output into something Python can easily understand\n",
    "        # using the '|' operator is basically the chaining part of the processing pipeline\n",
    "        # ^ this says the output of one component should be used as the input of the next component\n",
    "        # ^ so the prompt's output is the llm's input, the llm's output is the parser's input\n",
    "        \n",
    "        try: # we using a try-except bc who knows if the GPT will output something that is always understandable\n",
    "            ans = marketinGPT.invoke({\"prod\": prod_des}) # marketinGPT (brilliant name) is the name of the pipeline\n",
    "            # so when we call it, we r getting an instance of it\n",
    "            # .invoke(input val) is a method that tells the model to provide a response based on the input val\n",
    "            # \"prod\" is the input variable in the prompt, prod_des is the value in the for-each loop\n",
    "            # this lets prod_des be passed in as the input of the prompt\n",
    "            \n",
    "            parsed_response = parse_response(ans) # here we use parse_response to parse the response (DUH)\n",
    "            \n",
    "            ar1.append(parsed_response[0]) # here we r appending (adding) the answer to the arrays\n",
    "            ar2.append(parsed_response[1]) # hopefully everything is right\n",
    "            ar3.append(parsed_response[2]) # but otherwise it will all be missing descriptions\n",
    "            ar4.append(parsed_response[3]) # so this tells us if the GPT failed to generate an expected portion\n",
    "            \n",
    "        except Exception as e: # this is just the exception portion\n",
    "            print(f\"Couldn't process: {prod_des}. Error: {e}\") # we r just saying if we couldn't process any part of the inputs\n",
    "            ar1.append('could not process')\n",
    "            ar2.append('could not process')\n",
    "            ar3.append('could not process')\n",
    "            ar4.append('could not process')\n",
    "\n",
    "        t2 = time.time()\n",
    "        t = round(t2-t1)\n",
    "        i += 1\n",
    "        \n",
    "        print(f\"round {i} took {t} seconds\")\n",
    "        \n",
    "    return df_mk(ar1, ar2, ar3, ar4) # finally we make the df w the arrays we built\n",
    "\n",
    "# creating the df\n",
    "df = call_marketinGPT()\n",
    "\n",
    "# styling df\n",
    "t1 = time.time()\n",
    "\n",
    "styled_df = df.style.set_properties(**{ # so we're just making another df based on the generated df but w styling\n",
    "    'background-color': 'aliceblue', # the syntax for setting stuff is [what u wanna set]: [val]\n",
    "    'color': 'black',\n",
    "    'text-align': 'center',\n",
    "    'border': '2px solid lightsteelblue !important'\n",
    "}).set_table_styles([ # below, the selector means it applies to all the following, being thead, which is the table header section (thead)'s table headers (th)\n",
    "    {'selector': 'thead th', 'props': [('background-color', 'lightslategrey'), ('color', 'white'), ('border', '2px solid darkslategray !important'), ('text-align', 'center')]}\n",
    "]).hide(axis=\"index\")\n",
    "\n",
    "t2 = time.time()\n",
    "t = round(t2-t1)\n",
    "        \n",
    "print(f\"\\nstyling df took {t} seconds\")\n",
    "\n",
    "# displaying the styled df\n",
    "# display(df)\n",
    "display(styled_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
