{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round:  1\n",
      "there are  13  chunks\n",
      "there are  1024  elements in ctxt_embed\n",
      "there are  12  elements in search_embed\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 311\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_mk(ar1, ar2, ar3, ar4) \u001b[38;5;66;03m# finally we make the df w the arrays we built\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# creating the df\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcall_marketinGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# styling df\u001b[39;00m\n\u001b[1;32m    314\u001b[0m styled_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39mset_properties(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{ \u001b[38;5;66;03m# so we're just making another df based on the generated df but w styling\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackground-color\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maliceblue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# the syntax for setting stuff is [what u wanna set]: [val]\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselector\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthead th\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprops\u001b[39m\u001b[38;5;124m'\u001b[39m: [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackground-color\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightslategrey\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mborder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2px solid darkslategray !important\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-align\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m)]}\n\u001b[1;32m    321\u001b[0m ])\u001b[38;5;241m.\u001b[39mhide(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# ^ marked border as !important to make sure it's done bc it kept on NOT appearing??\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[66], line 266\u001b[0m, in \u001b[0;36mcall_marketinGPT\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_results) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    265\u001b[0m     min_ctxt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_results) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 266\u001b[0m top_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfiltered_results\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[:min_ctxt] \u001b[38;5;66;03m# this sorts the searches for highest matches\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# the zip() function combines two lists into a list of tuples made up of the results and their scores\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# sorted() sorts the function (DUH) based on \"key=lambda x: x[1]\" which means that it sorts on the second element (the scores)\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# reverse=True means that it's sorted in descending order rather than ascending so top scores r on the top\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# [:4] is the slice notation for getting the first up to :\"x\" index, so this would be the first 4 items\u001b[39;00m\n\u001b[1;32m    272\u001b[0m rag_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m top_results])\n",
      "Cell \u001b[0;32mIn[66], line 266\u001b[0m, in \u001b[0;36mcall_marketinGPT.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_results) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    265\u001b[0m     min_ctxt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_results) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 266\u001b[0m top_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mzip\u001b[39m(filtered_results), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:min_ctxt] \u001b[38;5;66;03m# this sorts the searches for highest matches\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# the zip() function combines two lists into a list of tuples made up of the results and their scores\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# sorted() sorts the function (DUH) based on \"key=lambda x: x[1]\" which means that it sorts on the second element (the scores)\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# reverse=True means that it's sorted in descending order rather than ascending so top scores r on the top\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# [:4] is the slice notation for getting the first up to :\"x\" index, so this would be the first 4 items\u001b[39;00m\n\u001b[1;32m    272\u001b[0m rag_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m top_results])\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate # for creating the template we feed to llm\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain.embeddings import QianfanEmbeddingsEndpoint\n",
    "# ^ for getting the actual GPT llm and also the embeddor\n",
    "from langchain_core.output_parsers import StrOutputParser # for converting llm output to something Python understands\n",
    "import pandas as pd # for making dfs\n",
    "from IPython.display import display # for displaying the df with styling\n",
    "import numpy as np # for doing dot product\n",
    "import requests # for making HTTP request in Python to handle headers, cookies, and authentication\n",
    "from bs4 import BeautifulSoup # for parsing HTML and XML docs so we can get j the urls / body text directly from a webpage\n",
    "import re # for using regex funcs like sub and shit\n",
    "\n",
    "# function to split text since the max length keeps being exceeded\n",
    "def split_text(text):\n",
    "    words = text.split() # splits the input string into a list of individual words\n",
    "    chunks = [] # stores final list of chunks\n",
    "    current_chunk = [] # temp list for each chunk\n",
    "\n",
    "    for word in words: # for each word\n",
    "        if len(\" \".join(current_chunk + [word])) <= 400 and word: # if adding this word doesn't exceed 2000 words\n",
    "            current_chunk.append(word) # then it is added\n",
    "        elif current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk)) # else the current chunk is added to the chunks list as a single string\n",
    "            current_chunk = [word] # then current_chunk is refreshed into a new chunk\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk)) # then at the end if there's a current chunk left then that's added to the list\n",
    "\n",
    "    return chunks # returns a list of chunks of max length 2000\n",
    "\n",
    "# does web scraping and text searching to search Baidu -> get search urls -> fetch content from urls -> process text\n",
    "def baidu_search(query): # query being the context we got\n",
    "    # defines url for baidu search\n",
    "    url = \"https://www.baidu.com/s\" # baidu (DUH)\n",
    "    \n",
    "    search_query = {'wd': query} # wd stands for word/query which is what we're searching (the context we're taking in)\n",
    "    \n",
    "    headers = { # headers r necessary bc when we as humans make searches we hv these; we need this to mimic a regular web browser's request\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    } # the User-Agent is a commonly used string for web scraping to avoid detection and blocking by webs\n",
    "    \n",
    "    # sends the get request to Baidu w our search params and header to do the search for relevant webpages\n",
    "    response = requests.get(url, params=search_query, headers=headers)\n",
    "    \n",
    "    # we need to initialize a Beautiful Soup object to actually parse the HTML so we can just get the urls\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # making a list to store urls of search results\n",
    "    results = []\n",
    "    # so a webpage will be separated in a bunch of divs as you know, and each div has a class that its associated w\n",
    "    # so when we do soup.find_all('div', class_='result'), we r using the soup object to search across all 'div's for the result class\n",
    "    # and then we r doing a for each loop of all the result divs and finding the link portion of it\n",
    "    for item in soup.find_all('div', class_='result'):\n",
    "        link = item.find('a', href=True) # 'a' is a link notation\n",
    "        if link:\n",
    "            results.append(link['href']) # and if it's a link then we add it into the list w the 'href' attribute which is a url\n",
    "\n",
    "    # now we are making another list based on our results list that takes all the urls there and calls get_page on it to get the acc page\n",
    "    docs = get_page(results) # so this stores a bunch of pages\n",
    "\n",
    "    # making another list to store the acc content of the pages in\n",
    "    content = []\n",
    "    for doc in docs: # for each doc in the docs list\n",
    "        page_text = re.sub(\"\\n\\n+\", \"\\n\", doc) # this regex part is p cool, sub() obviously substitutes something w another thing\n",
    "        # and in this case we r subbing all the \\n\\n+ w j one \\n which means that anything more than 1 newline will be replaced w a newline\n",
    "        # so we can just get all the text in a braindead way like this\n",
    "        if page_text and page_text != \"问题反馈\": # checks to see if empty string or bad text\n",
    "            # ^ note that in Python, an empty string of \"\" returns False\n",
    "            # page_text = cutoff(page_text)\n",
    "            content.append(page_text) # and we r adding it to the list\n",
    "        \n",
    "    # i = 0\n",
    "    # for ctnt in content:\n",
    "    #     i += 1\n",
    "    #     print(\"entry \", i,\": \",ctnt)\n",
    "\n",
    "    return content # and now we r just returning a list of all the text from relevant pages\n",
    "\n",
    "# this is the function that we called in baidu_search to acc get the webpages from the urls\n",
    "def get_page(urls):\n",
    "    \n",
    "    docs = [] # we r making to store the page contents\n",
    "    # similar process w headers as above\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    # a for each loop of all the urls\n",
    "    for url in urls:\n",
    "        \n",
    "        response = requests.get(url, headers=headers) # this time we r j popping into all the pages\n",
    "        \n",
    "        if response.status_code == 200: # status code 200 means that it was successful so if we were let in, then we call beautiful soup\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser') # we initiate the soup object again\n",
    "            # ^ we get the content from the response (the response being the page that we popped into)\n",
    "            # ^ then we specify we wanna use 'html.parser' to get the info\n",
    "            paragraphs = soup.find_all('p') # 'p' = praragraph so now we r making a list of all the paragraphs from the parsed html\n",
    "            # ^ the issue is what if there are text content thats important thats not marked as 'p' or what if 'p' isn't right?\n",
    "            page_text = \"\\n\".join([p.get_text() for p in paragraphs]) # and we extract all the text from the paragraphs & join them\n",
    "            chunks = split_text(page_text) # so that it doesn't keep causing errors\n",
    "            docs.extend(chunks) # and now we add the text into our list of page content\n",
    "    # so finally we can return a list w all the page contents of j all the text on the page\n",
    "    return docs\n",
    "\n",
    "# this is just a function to cut off excess words cuz there's a max that can be embedded\n",
    "# def cutoff(text):\n",
    "#     words = text.split() # this splits all the text into a list of individual word strings\n",
    "#     truncated = \" \".join(words[:742]) # and now we r just joining the words tgt (2000 is max)\n",
    "#     return truncated\n",
    "\n",
    "# key\n",
    "qianfan_ak = \"DAEEqjuvglLTgQMCXqRvqfUj\"\n",
    "qianfan_sk = \"s0AJ849GNB6440lwLWDvGuNEJNrgrbQ3\"\n",
    "\n",
    "# models\n",
    "llm = QianfanChatEndpoint(model=\"ERNIE-4.0-8K\", streaming=True, qianfan_ak=qianfan_ak, qianfan_sk=qianfan_sk, penalty_score=1)\n",
    "embed = QianfanEmbeddingsEndpoint(model=\"bge_large_zh\", endpoint=\"bge_large_zh\", qianfan_ak=qianfan_ak, qianfan_sk=qianfan_sk)\n",
    "\n",
    "# does the rag search and returns a list (of strings) with all the info from the first few pages of web links (each link's info is concatenated tgt)\n",
    "def rag_search(query):\n",
    "    return baidu_search(query) # rn we hv them as diff functions in case baidu doesn't work out\n",
    "\n",
    "def df_mk(ar1, ar2, ar3, ar4):\n",
    "    df = pd.DataFrame({ # this is the syntax for making a df which is basically a table in pandas\n",
    "        \"产品描述\": ar1, # the quotes has the title of the column\n",
    "        \"产品卖点\": ar2, # u can either make an array like [val, val] urself\n",
    "        \"最佳营销卖点\": ar3, # or u can use an array that u alr made\n",
    "        \"目标受众\": ar4 # and put it into the df like that\n",
    "    })\n",
    "        \n",
    "    return df\n",
    "\n",
    "def parse_response(llm_output): # this parses the output param that is generated by the llm to find the info we need for the df\n",
    "    \n",
    "    # split the output into lines\n",
    "    lines = llm_output.split('\\n') # split function is a python func that turns a string into a list\n",
    "    # ^ where each newline from the og where the items r separated\n",
    "    \n",
    "    # initialize placeholders\n",
    "    product_description = 'missing description' # this is for debugging in case we find something isn't generated properly\n",
    "    selling_points = 'missing description' # then we know that whatever has \"missing description\" didn't have that part\n",
    "    best_marketing_point = 'missing description'\n",
    "    target_audience = 'missing description'\n",
    "    \n",
    "    # iterate over lines and find the relevant sections\n",
    "    for line in lines: # for each loop that searches each line in the output for the content we need\n",
    "        if line.startswith(\"1. **产品描述**：\"): # so if the line starts with [this] then we store this line into the corresponding var\n",
    "            product_description = line[len(\"1. **产品描述**：\"):].strip() # but we strip the title for cleanliness\n",
    "        elif line.startswith(\"2. **产品卖点**：\"):\n",
    "            selling_points = line[len(\"2. **产品卖点**：\"):].strip()\n",
    "        elif line.startswith(\"3. **最佳营销卖点**：\"):\n",
    "            best_marketing_point = line[len(\"3. **最佳营销卖点**：\"):].strip()\n",
    "        elif line.startswith(\"4. **目标受众**：\"):\n",
    "            target_audience = line[len(\"4. **目标受众**：\"):].strip()\n",
    "    \n",
    "    return product_description, selling_points, best_marketing_point, target_audience # and we just return the variables\n",
    "\n",
    "\n",
    "def call_marketinGPT():\n",
    "\n",
    "    file_path = \"products.txt\" # this is the file u wanna open\n",
    "    # if it was in a diff path, then u would hv to do ../folder/folder/file.txt instead\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file: # 'r' means read (DUH), utf-8 encoding is standard\n",
    "        prod_descr = file.readlines() # \"with\" makes sure it's closed at the end\n",
    "    # \"as file\" basically sets what's opened into the variable \"file\"\n",
    "    # prod_descr is a list type var that stores everything that is read from the file\n",
    "    # readlines reads all the individual lines (broken apart by \\n) into the list\n",
    "    \n",
    "    prod_descr = [desc.strip() for desc in prod_descr] # prod_descr prior to this would be like [\"prod_des1\\n\", \"prod_des2\\n\"]\n",
    "    # desc is each individual line stored in prod_descr, using desc.strip for each desc gets rid of \\n at the end of each one\n",
    "    # now, prod_descr is like [\"prod_des1\", \"prod_des2\"]\n",
    "    \n",
    "    prompt1 = PromptTemplate(\n",
    "        # template is the prompt that ur using to prompt engineer the GPT\n",
    "                \n",
    "        template = \"\"\"输入一个产品名称后，生成一段简短描述，涵盖其主要卖点、特点和优势。\\n\\n\n",
    "\n",
    "        输入格式：\\n\n",
    "        [{prod}]\\n\\n\n",
    "\n",
    "        输出格式：\\n\n",
    "        [简短描述，包括卖点、特点和优势]\\n\"\"\",\n",
    "        \n",
    "        input_variables = [\"prod\"] # here ur telling the gpt that the input variables it uses will be\n",
    "        # used where {prod} is used in the template\n",
    "    )\n",
    "    \n",
    "    prompt2 = PromptTemplate(\n",
    "                        \n",
    "        template=\"\"\"作为一名零售顾问助手，你的任务是帮助用户分析他们的产品描述，\n",
    "        并提供该产品的卖点、最佳营销卖点、目标受众以及针对目标受众的营销策略。\n",
    "        请根据以下格式进行回复，并且仅根据用户提供的信息进行分析和回答：\\n\\n\n",
    "            1. **产品描述**：用户提供的产品详细信息。\\n\n",
    "            2. **产品卖点**：根据产品描述，提炼出吸引潜在消费者的关键特点。\\n\n",
    "            3. **最佳营销卖点**：从产品卖点中选择最具市场潜力的特点，并解释为何这个卖点最有吸引力。\\n\n",
    "            4. **目标受众**：根据产品卖点，确定最适合的消费群体。\\n\\n\n",
    "            \n",
    "        以下是一个示例对话：\\n\n",
    "        \n",
    "        用户：我们有一款新型的可折叠电动自行车，重量轻，电池续航长，适合城市通勤。\\n\\n\n",
    "        系统：\\n\n",
    "            1. **产品描述**：新型可折叠电动自行车，重量轻，电池续航长，适合城市通勤。\\n\n",
    "            2. **产品卖点**：轻便设计、长续航电池、便捷的城市通勤工具。\\n\n",
    "            3. **最佳营销卖点**：长续航电池，因为城市通勤用户对续航时间有较高需求，能够减少充电频率。\\n\n",
    "            4. **目标受众**：城市白领、大学生、注重环保和便捷出行的用户。\\n\\n\n",
    "            \n",
    "        请提供您的产品描述：\\n\n",
    "        \n",
    "        {prod}\\n\\n\n",
    "\n",
    "        1. **产品描述**：用户提供的产品描述\\n\n",
    "        2. **产品卖点**：提炼出的产品卖点\\n\n",
    "        3. **最佳营销卖点**：选择的最佳营销卖点及其原因\\n\n",
    "        4. **目标受众**：确定的目标消费群体\\n\n",
    "        \n",
    "        主要使用以下信息来得出答案。即使以下信息提到其他具体产品，也要专注于该产品的优点，不提及其他产品的名称。：\\n\\n\n",
    "        \n",
    "        {context}\"\"\",\n",
    "        \n",
    "        input_variables = [\"prod\", \"context\"] # here ur telling the gpt that the input variables it uses will be\n",
    "    )\n",
    "    \n",
    "    ar1, ar2, ar3, ar4 = [], [], [], [] # here ur declaring the arrays that the df will be made w\n",
    "    \n",
    "    i = 0\n",
    "    for prod_des in prod_descr: # this is a for-each loop which ensures that each val in the list is used\n",
    "        \n",
    "        i += 1\n",
    "        print(\"round: \", i)\n",
    "        \n",
    "        marketinGPT = prompt1 | llm | StrOutputParser()\n",
    "        \n",
    "        ctxt = marketinGPT.invoke({\"prod\": prod_des})\n",
    "        \n",
    "        # do RAG search and embed the context and search results\n",
    "        ctxt_embed = embed.embed_query(ctxt) # uses the langchain qianfan embedder\n",
    "        search_results = rag_search(ctxt) # calls the RAG search func to get the list of search results for the context we r looking for\n",
    "        \n",
    "        all_chunks = [] # makes a list to store the chunks\n",
    "        for result in search_results: # for every single result we get\n",
    "            chunks = split_text(result) # we split the result into a list of chunks\n",
    "            all_chunks.extend(chunks) # and adds all the chunks to the list separately\n",
    "            # ^ that's the diff between extend (which adds all the chunks separately) vs append (which would turn it into a list of lists of chunks)\n",
    "        \n",
    "        print(\"there are \", len(all_chunks),  \" chunks\")\n",
    "        \n",
    "        search_embed = []\n",
    "        i1 = 0\n",
    "        i2 = 0\n",
    "        while i2 < len(all_chunks) - 1:\n",
    "            i2 += 1\n",
    "            search_embed.extend(embed.embed_documents(all_chunks[i1:i2])) # then embeds the search results asw\n",
    "            i1 += 1\n",
    "        \n",
    "        # calculate dot product and get closest results\n",
    "        search_embed = np.array(search_embed)\n",
    "        \n",
    "        print(\"there are \", len(ctxt_embed),  \" elements in ctxt_embed\")\n",
    "        print(\"there are \", len(search_embed),  \" elements in search_embed\")\n",
    "\n",
    "        \n",
    "        similarity_scores = np.dot(ctxt_embed, search_embed.T) # this will get all the dot products for our searches X context\n",
    "        filtered_results = [(result, score) for result, score in zip(search_results, similarity_scores) if score > 0.5]\n",
    "        min_ctxt = 3\n",
    "        if len(filtered_results) < 3:\n",
    "            min_ctxt = len(filtered_results) - 1\n",
    "        \n",
    "        print(\"the num of the filtered results is \", len(filtered_results), \" while the min_ctxt is \", min_ctxt)\n",
    "        \n",
    "        top_results = sorted(zip(filtered_results), key=lambda x: x[1], reverse=True)[:min_ctxt] # this sorts the searches for highest matches\n",
    "        # the zip() function combines two lists into a list of tuples made up of the results and their scores\n",
    "        # sorted() sorts the function (DUH) based on \"key=lambda x: x[1]\" which means that it sorts on the second element (the scores)\n",
    "        # reverse=True means that it's sorted in descending order rather than ascending so top scores r on the top\n",
    "        # [:4] is the slice notation for getting the first up to :\"x\" index, so this would be the first 4 items\n",
    "        \n",
    "        rag_results = \" \".join([result[0] for result in top_results])\n",
    "        \n",
    "        print(\"the context we are giving is: \", rag_results)\n",
    "        \n",
    "        marketinGPT = prompt2 | llm | StrOutputParser() # this is the setup for the processing pipeline\n",
    "        # prompt refers to the template ur using to prompt engineer -> this is given to llm\n",
    "        # llm then takes the text input and generates a response\n",
    "        # StrOutputParser is an output parser (DUH but also an output parser takes raw output and turns it into a structured format)\n",
    "        # ^ this turns the llm's output into something Python can easily understand\n",
    "        # using the '|' operator is basically the chaining part of the processing pipeline\n",
    "        # ^ this says the output of one component should be used as the input of the next component\n",
    "        # ^ so the prompt's output is the llm's input, the llm's output is the parser's input\n",
    "        \n",
    "        try: # we using a try-except bc who knows if the GPT will output something that is always understandable\n",
    "                        \n",
    "            ans = marketinGPT.invoke({\"prod\": prod_des, \"context\": rag_results}) # marketinGPT (brilliant name) is the name of the pipeline\n",
    "            # so when we call it, we r getting an instance of it\n",
    "            # .invoke(input val) is a method that tells the model to provide a response based on the input val\n",
    "            # \"prod\" is the input variable in the prompt, prod_des is the value in the for-each loop\n",
    "            # this lets prod_des be passed in as the input of the prompt\n",
    "            # ^ same for rag_result\n",
    "            \n",
    "            parsed_response = parse_response(ans) # here we use parse_response to parse the response (DUH)\n",
    "            \n",
    "            ar1.append(parsed_response[0]) # here we r appending (adding) the answer to the arrays\n",
    "            ar2.append(parsed_response[1]) # hopefully everything is right\n",
    "            ar3.append(parsed_response[2]) # but otherwise it will all be missing descriptions\n",
    "            ar4.append(parsed_response[3]) # so this tells us if the GPT failed to generate an expected portion\n",
    "            \n",
    "        except Exception as e: # this is just the exception portion\n",
    "            print(f\"Couldn't process: {prod_des}. Error: {e}\") # we r just saying if we couldn't process any part of the inputs\n",
    "            ar1.append('could not process')\n",
    "            ar2.append('could not process')\n",
    "            ar3.append('could not process')\n",
    "            ar4.append('could not process')\n",
    "\n",
    "    return df_mk(ar1, ar2, ar3, ar4) # finally we make the df w the arrays we built\n",
    "\n",
    "# creating the df\n",
    "df = call_marketinGPT()\n",
    "\n",
    "# styling df\n",
    "styled_df = df.style.set_properties(**{ # so we're just making another df based on the generated df but w styling\n",
    "    'background-color': 'aliceblue', # the syntax for setting stuff is [what u wanna set]: [val]\n",
    "    'color': 'black',\n",
    "    'text-align': 'center',\n",
    "    'border': '2px solid lightsteelblue !important'\n",
    "}).set_table_styles([ # below, the selector means it applies to all the following, being thead, which is the table header section (thead)'s table headers (th)\n",
    "    {'selector': 'thead th', 'props': [('background-color', 'lightslategrey'), ('color', 'white'), ('border', '2px solid darkslategray !important'), ('text-align', 'center')]}\n",
    "]).hide(axis=\"index\") # ^ marked border as !important to make sure it's done bc it kept on NOT appearing??\n",
    "\n",
    "# displaying the styled df\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1e49a thead th {\n",
       "  background-color: lightslategrey;\n",
       "  color: white;\n",
       "  border: 2px solid darkslategray !important;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_1e49a_row0_col0, #T_1e49a_row0_col1, #T_1e49a_row0_col2, #T_1e49a_row0_col3, #T_1e49a_row1_col0, #T_1e49a_row1_col1, #T_1e49a_row1_col2, #T_1e49a_row1_col3, #T_1e49a_row2_col0, #T_1e49a_row2_col1, #T_1e49a_row2_col2, #T_1e49a_row2_col3, #T_1e49a_row3_col0, #T_1e49a_row3_col1, #T_1e49a_row3_col2, #T_1e49a_row3_col3, #T_1e49a_row4_col0, #T_1e49a_row4_col1, #T_1e49a_row4_col2, #T_1e49a_row4_col3 {\n",
       "  background-color: aliceblue;\n",
       "  color: black;\n",
       "  text-align: center;\n",
       "  border: 2px solid lightsteelblue !important;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1e49a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_1e49a_level0_col0\" class=\"col_heading level0 col0\" >产品描述</th>\n",
       "      <th id=\"T_1e49a_level0_col1\" class=\"col_heading level0 col1\" >产品卖点</th>\n",
       "      <th id=\"T_1e49a_level0_col2\" class=\"col_heading level0 col2\" >最佳营销卖点</th>\n",
       "      <th id=\"T_1e49a_level0_col3\" class=\"col_heading level0 col3\" >目标受众</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_1e49a_row0_col0\" class=\"data row0 col0\" >Midea Ultimate Comfort Series - 美的终极舒适系列，致力于为用户提供极致的舒适体验。系列产品包括高性能空调、智能热水器以及多功能空气净化器，均采用先进的节能技术，不仅功能强大而且操作简单，满足现代家庭对高品质生活的追求。</td>\n",
       "      <td id=\"T_1e49a_row0_col1\" class=\"data row0 col1\" >极致舒适体验、高性能空调、智能热水器、多功能空气净化器、先进节能技术、强大功能、简单操作、满足高品质生活追求。</td>\n",
       "      <td id=\"T_1e49a_row0_col2\" class=\"data row0 col2\" >极致舒适体验。这个卖点凸显了产品的核心理念，即为用户提供无与伦比的舒适感。在现代社会，人们对家居生活的舒适度要求越来越高，这一卖点能够直接触及消费者的核心需求，吸引他们购买。</td>\n",
       "      <td id=\"T_1e49a_row0_col3\" class=\"data row0 col3\" >追求高品质生活的中高端消费者，包括注重家居舒适度的家庭用户、对智能家电有较高要求的年轻人群，以及关心节能环保的环保意识较强的消费者。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1e49a_row1_col0\" class=\"data row1 col0\" >Haier HB18FGSAAA - 海尔 HB18FGSAAA，一款高性能的冰箱，采用了先进的冷藏技术，确保食物长时间新鲜。其大容量设计，可轻松存储全家人的食材需求。同时，这款冰箱运行静音，节能省电，为家庭创造宁静且环保的生活环境。简洁时尚的外观设计，能够融入各种家居风格，提升整体家居美感。</td>\n",
       "      <td id=\"T_1e49a_row1_col1\" class=\"data row1 col1\" >先进冷藏技术、大容量存储、静音运行、节能省电、时尚外观设计。</td>\n",
       "      <td id=\"T_1e49a_row1_col2\" class=\"data row1 col2\" >先进冷藏技术，因为保鲜是冰箱最核心的功能，而先进的技术能确保食物长时间新鲜，满足消费者对食品质量和健康的高要求。</td>\n",
       "      <td id=\"T_1e49a_row1_col3\" class=\"data row1 col3\" >追求高品质生活的家庭，注重食品健康和保鲜的消费者，喜欢时尚家居设计的用户。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1e49a_row2_col0\" class=\"data row2 col0\" >Lenovo Yoga 9i Gen 8 - 联想 Yoga 9i 第八代，是一款兼具高性能与多模式转换的笔记本电脑。它搭载最新一代处理器，提供强劲的计算能力，同时其独特的360度翻转设计，让用户可以根据需求轻松切换笔记本、平板、帐篷等多种使用模式。此外，高清触控屏带来出色的视觉体验，适合各种办公与娱乐场景。</td>\n",
       "      <td id=\"T_1e49a_row2_col1\" class=\"data row2 col1\" >高性能处理器、多模式转换设计、高清触控屏、适应多种场景。</td>\n",
       "      <td id=\"T_1e49a_row2_col2\" class=\"data row2 col2\" >多模式转换设计。这一特点使得Lenovo Yoga 9i Gen 8不仅仅是一台传统的笔记本电脑，更能满足用户在不同场合下的多样化需求。无论是正式的办公会议，还是休闲娱乐，都能通过简单的翻转动作找到最舒适的使用方式。</td>\n",
       "      <td id=\"T_1e49a_row2_col3\" class=\"data row2 col3\" >商务人士、创意工作者、对科技产品有较高要求的学生群体，以及希望拥有一台既实用又具备多功能性的笔记本电脑的消费者。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1e49a_row3_col0\" class=\"data row3 col0\" >Xiaomi Mi Band 8 - 小米手环8，是一款功能全面的智能手环。它拥有出色的健康监测功能，包括心率检测、血氧检测、睡眠跟踪等，帮助用户全面了解自己的身体状况。此外，小米手环8还支持多种运动模式，能够精确记录运动数据，助力用户达成健身目标。其时尚的外观设计和长久的电池续航也是产品的亮点，让用户在享受智能科技带来的便利的同时，也能展现自己的个性魅力。</td>\n",
       "      <td id=\"T_1e49a_row3_col1\" class=\"data row3 col1\" >功能全面、健康监测、多种运动模式、时尚外观、长久电池续航。</td>\n",
       "      <td id=\"T_1e49a_row3_col2\" class=\"data row3 col2\" >功能全面，因为小米手环8不仅提供了健康监测功能，还支持多种运动模式，满足了消费者对于智能手环多元化的需求。这种一站式的服务能够吸引更广泛的消费者群体，提升产品的市场竞争力。</td>\n",
       "      <td id=\"T_1e49a_row3_col3\" class=\"data row3 col3\" >注重健康管理的年轻人、经常进行运动的人群、追求时尚科技的消费者。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1e49a_row4_col0\" class=\"data row4 col0\" >Anta KT7 - 安踏 KT7，专业篮球鞋款，采用高科技缓震材料，提供卓越的弹跳支撑。鞋面采用透气材质，确保脚部干爽舒适。抗滑耐磨的鞋底设计，适应各种球场地面，助力球员发挥最佳表现。</td>\n",
       "      <td id=\"T_1e49a_row4_col1\" class=\"data row4 col1\" >专业篮球鞋款、高科技缓震材料、卓越弹跳支撑、透气鞋面、抗滑耐磨鞋底。</td>\n",
       "      <td id=\"T_1e49a_row4_col2\" class=\"data row4 col2\" >卓越弹跳支撑，因为篮球运动员对于鞋子的弹跳性能要求极高，这一卖点直接关联到球员在比赛中的表现，能够满足专业球员对鞋子功能性的核心需求。</td>\n",
       "      <td id=\"T_1e49a_row4_col3\" class=\"data row4 col3\" >专业篮球运动员、篮球爱好者、追求高性能篮球鞋的消费者。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17404a8b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate # for creating the template we feed to llm\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain.embeddings import QianfanEmbeddingsEndpoint\n",
    "# ^ for getting the actual GPT llm and also the embeddor\n",
    "from langchain_core.output_parsers import StrOutputParser # for converting llm output to something Python understands\n",
    "import pandas as pd # for making dfs\n",
    "from IPython.display import display # for displaying the df with styling\n",
    "import numpy as np # for doing dot product\n",
    "import requests # for making HTTP request in Python to handle headers, cookies, and authentication\n",
    "from bs4 import BeautifulSoup # for parsing HTML and XML docs so we can get j the urls / body text directly from a webpage\n",
    "import re # for using regex funcs like sub and shit\n",
    "\n",
    "# key\n",
    "qianfan_ak = \"DAEEqjuvglLTgQMCXqRvqfUj\"\n",
    "qianfan_sk = \"s0AJ849GNB6440lwLWDvGuNEJNrgrbQ3\"\n",
    "\n",
    "# models\n",
    "llm = QianfanChatEndpoint(model=\"ERNIE-4.0-8K\", streaming=True, qianfan_ak=qianfan_ak, qianfan_sk=qianfan_sk, penalty_score=1)\n",
    "embed = QianfanEmbeddingsEndpoint(model=\"bge_large_zh\", endpoint=\"bge_large_zh\", qianfan_ak=qianfan_ak, qianfan_sk=qianfan_sk)\n",
    "\n",
    "def df_mk(ar1, ar2, ar3, ar4):\n",
    "    df = pd.DataFrame({ # this is the syntax for making a df which is basically a table in pandas\n",
    "        \"产品描述\": ar1, # the quotes has the title of the column\n",
    "        \"产品卖点\": ar2, # u can either make an array like [val, val] urself\n",
    "        \"最佳营销卖点\": ar3, # or u can use an array that u alr made\n",
    "        \"目标受众\": ar4 # and put it into the df like that\n",
    "    })\n",
    "        \n",
    "    return df\n",
    "\n",
    "def parse_response(llm_output): # this parses the output param that is generated by the llm to find the info we need for the df\n",
    "    \n",
    "    # split the output into lines\n",
    "    lines = llm_output.split('\\n') # split function is a python func that turns a string into a list\n",
    "    # ^ where each newline from the og where the items r separated\n",
    "    \n",
    "    # initialize placeholders\n",
    "    product_description = 'missing description' # this is for debugging in case we find something isn't generated properly\n",
    "    selling_points = 'missing description' # then we know that whatever has \"missing description\" didn't have that part\n",
    "    best_marketing_point = 'missing description'\n",
    "    target_audience = 'missing description'\n",
    "    \n",
    "    # iterate over lines and find the relevant sections\n",
    "    for line in lines: # for each loop that searches each line in the output for the content we need\n",
    "        if line.startswith(\"1. **产品描述**：\"): # so if the line starts with [this] then we store this line into the corresponding var\n",
    "            product_description = line[len(\"1. **产品描述**：\"):].strip() # but we strip the title for cleanliness\n",
    "        elif line.startswith(\"2. **产品卖点**：\"):\n",
    "            selling_points = line[len(\"2. **产品卖点**：\"):].strip()\n",
    "        elif line.startswith(\"3. **最佳营销卖点**：\"):\n",
    "            best_marketing_point = line[len(\"3. **最佳营销卖点**：\"):].strip()\n",
    "        elif line.startswith(\"4. **目标受众**：\"):\n",
    "            target_audience = line[len(\"4. **目标受众**：\"):].strip()\n",
    "    \n",
    "    return product_description, selling_points, best_marketing_point, target_audience # and we just return the variables\n",
    "\n",
    "\n",
    "def call_marketinGPT():\n",
    "\n",
    "    file_path = \"products.txt\" # this is the file u wanna open\n",
    "    # if it was in a diff path, then u would hv to do ../folder/folder/file.txt instead\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file: # 'r' means read (DUH), utf-8 encoding is standard\n",
    "        prod_descr = file.readlines() # \"with\" makes sure it's closed at the end\n",
    "    # \"as file\" basically sets what's opened into the variable \"file\"\n",
    "    # prod_descr is a list type var that stores everything that is read from the file\n",
    "    # readlines reads all the individual lines (broken apart by \\n) into the list\n",
    "    \n",
    "    prod_descr = [desc.strip() for desc in prod_descr] # prod_descr prior to this would be like [\"prod_des1\\n\", \"prod_des2\\n\"]\n",
    "    # desc is each individual line stored in prod_descr, using desc.strip for each desc gets rid of \\n at the end of each one\n",
    "    # now, prod_descr is like [\"prod_des1\", \"prod_des2\"]\n",
    "    \n",
    "    prompt2 = PromptTemplate(\n",
    "                        \n",
    "        template=\"\"\"作为一名零售顾问助手，你的任务是帮助用户分析他们的产品描述，\n",
    "        并提供该产品的卖点、最佳营销卖点、目标受众以及针对目标受众的营销策略。\n",
    "        请根据以下格式进行回复，并且仅根据用户提供的信息进行分析和回答：\\n\\n\n",
    "            1. **产品描述**：用户提供的产品详细信息。\\n\n",
    "            2. **产品卖点**：根据产品描述，提炼出吸引潜在消费者的关键特点。\\n\n",
    "            3. **最佳营销卖点**：从产品卖点中选择最具市场潜力的特点，并解释为何这个卖点最有吸引力。\\n\n",
    "            4. **目标受众**：根据产品卖点，确定最适合的消费群体。\\n\\n\n",
    "            \n",
    "        以下是一个示例对话：\\n\n",
    "        \n",
    "        用户：我们有一款新型的可折叠电动自行车，重量轻，电池续航长，适合城市通勤。\\n\\n\n",
    "        系统：\\n\n",
    "            1. **产品描述**：新型可折叠电动自行车，重量轻，电池续航长，适合城市通勤。\\n\n",
    "            2. **产品卖点**：轻便设计、长续航电池、便捷的城市通勤工具。\\n\n",
    "            3. **最佳营销卖点**：长续航电池，因为城市通勤用户对续航时间有较高需求，能够减少充电频率。\\n\n",
    "            4. **目标受众**：城市白领、大学生、注重环保和便捷出行的用户。\\n\\n\n",
    "            \n",
    "        请提供您的产品描述：\\n\n",
    "        \n",
    "        {prod}\\n\\n\n",
    "\n",
    "        1. **产品描述**：用户提供的产品描述\\n\n",
    "        2. **产品卖点**：提炼出的产品卖点\\n\n",
    "        3. **最佳营销卖点**：选择的最佳营销卖点及其原因\\n\n",
    "        4. **目标受众**：确定的目标消费群体\\n\"\"\",\n",
    "        \n",
    "        input_variables = [\"prod\", \"context\"] # here ur telling the gpt that the input variables it uses will be\n",
    "    )\n",
    "    \n",
    "    ar1, ar2, ar3, ar4 = [], [], [], [] # here ur declaring the arrays that the df will be made w\n",
    "\n",
    "    for prod_des in prod_descr: # this is a for-each loop which ensures that each val in the list is used\n",
    "        \n",
    "        marketinGPT = prompt2 | llm | StrOutputParser() # this is the setup for the processing pipeline\n",
    "        # prompt refers to the template ur using to prompt engineer -> this is given to llm\n",
    "        # llm then takes the text input and generates a response\n",
    "        # StrOutputParser is an output parser (DUH but also an output parser takes raw output and turns it into a structured format)\n",
    "        # ^ this turns the llm's output into something Python can easily understand\n",
    "        # using the '|' operator is basically the chaining part of the processing pipeline\n",
    "        # ^ this says the output of one component should be used as the input of the next component\n",
    "        # ^ so the prompt's output is the llm's input, the llm's output is the parser's input\n",
    "        \n",
    "        try: # we using a try-except bc who knows if the GPT will output something that is always understandable\n",
    "                        \n",
    "            ans = marketinGPT.invoke({\"prod\": prod_des}) # marketinGPT (brilliant name) is the name of the pipeline\n",
    "            # so when we call it, we r getting an instance of it\n",
    "            # .invoke(input val) is a method that tells the model to provide a response based on the input val\n",
    "            # \"prod\" is the input variable in the prompt, prod_des is the value in the for-each loop\n",
    "            # this lets prod_des be passed in as the input of the prompt\n",
    "            # ^ same for rag_result\n",
    "            \n",
    "            parsed_response = parse_response(ans) # here we use parse_response to parse the response (DUH)\n",
    "            \n",
    "            ar1.append(parsed_response[0]) # here we r appending (adding) the answer to the arrays\n",
    "            ar2.append(parsed_response[1]) # hopefully everything is right\n",
    "            ar3.append(parsed_response[2]) # but otherwise it will all be missing descriptions\n",
    "            ar4.append(parsed_response[3]) # so this tells us if the GPT failed to generate an expected portion\n",
    "            \n",
    "        except Exception as e: # this is just the exception portion\n",
    "            print(f\"Couldn't process: {prod_des}. Error: {e}\") # we r just saying if we couldn't process any part of the inputs\n",
    "            ar1.append('could not process')\n",
    "            ar2.append('could not process')\n",
    "            ar3.append('could not process')\n",
    "            ar4.append('could not process')\n",
    "\n",
    "    return df_mk(ar1, ar2, ar3, ar4) # finally we make the df w the arrays we built\n",
    "\n",
    "# creating the df\n",
    "df = call_marketinGPT()\n",
    "\n",
    "# styling df\n",
    "styled_df = df.style.set_properties(**{ # so we're just making another df based on the generated df but w styling\n",
    "    'background-color': 'aliceblue', # the syntax for setting stuff is [what u wanna set]: [val]\n",
    "    'color': 'black',\n",
    "    'text-align': 'center',\n",
    "    'border': '2px solid lightsteelblue !important'\n",
    "}).set_table_styles([ # below, the selector means it applies to all the following, being thead, which is the table header section (thead)'s table headers (th)\n",
    "    {'selector': 'thead th', 'props': [('background-color', 'lightslategrey'), ('color', 'white'), ('border', '2px solid darkslategray !important'), ('text-align', 'center')]}\n",
    "]).hide(axis=\"index\") # ^ marked border as !important to make sure it's done bc it kept on NOT appearing??\n",
    "\n",
    "# displaying the styled df\n",
    "display(styled_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
