{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 281\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_mk(ar1, ar2, ar3, ar4) \u001b[38;5;66;03m# finally we make the df w the arrays we built\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# creating the df\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcall_marketinGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# styling df\u001b[39;00m\n\u001b[1;32m    284\u001b[0m styled_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39mset_properties(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{ \u001b[38;5;66;03m# so we're just making another df based on the generated df but w styling\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackground-color\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maliceblue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# the syntax for setting stuff is [what u wanna set]: [val]\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselector\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthead th\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprops\u001b[39m\u001b[38;5;124m'\u001b[39m: [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackground-color\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightslategrey\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mborder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2px solid darkslategray !important\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-align\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m)]}\n\u001b[1;32m    291\u001b[0m ])\u001b[38;5;241m.\u001b[39mhide(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# ^ marked border as !important to make sure it's done bc it kept on NOT appearing??\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 233\u001b[0m, in \u001b[0;36mcall_marketinGPT\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# do RAG search and embed the context and search results\u001b[39;00m\n\u001b[1;32m    232\u001b[0m ctxt_embed \u001b[38;5;241m=\u001b[39m embed\u001b[38;5;241m.\u001b[39membed_query(ctxt) \u001b[38;5;66;03m# uses the langchain qianfan embedder\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m search_results \u001b[38;5;241m=\u001b[39m \u001b[43mrag_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctxt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# calls the RAG search func to get the list of search results for the context we r looking for\u001b[39;00m\n\u001b[1;32m    234\u001b[0m search_embed \u001b[38;5;241m=\u001b[39m embed\u001b[38;5;241m.\u001b[39membed_documents(search_results) \u001b[38;5;66;03m# then embeds the search results asw\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# calculate dot product and get closest results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 120\u001b[0m, in \u001b[0;36mrag_search\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrag_search\u001b[39m(query):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbaidu_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 65\u001b[0m, in \u001b[0;36mbaidu_search\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     63\u001b[0m content \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs: \u001b[38;5;66;03m# for each doc in the docs list\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     page_text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# this regex part is p cool, sub() obviously substitutes something w another thing\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# and in this case we r subbing all the \\n\\n+ w j one \\n which means that anything more than 1 newline will be replaced w a newline\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# so we can just get all the text in a braindead way like this\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m page_text \u001b[38;5;129;01mand\u001b[39;00m page_text \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m问题反馈\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;66;03m# checks to see if empty string or bad text\u001b[39;00m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# ^ note that in Python, an empty string of \"\" returns False\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/re.py:210\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate # for creating the template we feed to llm\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain.embeddings import QianfanEmbeddingsEndpoint\n",
    "# ^ for getting the actual GPT llm and also the embeddor\n",
    "from langchain_core.output_parsers import StrOutputParser # for converting llm output to something Python understands\n",
    "import pandas as pd # for making dfs\n",
    "from IPython.display import display # for displaying the df with styling\n",
    "import numpy as np # for doing dot product\n",
    "import requests # for making HTTP request in Python to handle headers, cookies, and authentication\n",
    "from bs4 import BeautifulSoup # for parsing HTML and XML docs so we can get j the urls / body text directly from a webpage\n",
    "import re # for using regex funcs like sub and shit\n",
    "\n",
    "# function to split text since the max length keeps being exceeded\n",
    "def split_text(text):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(\" \".join(current_chunk + [word])) <= 2000:\n",
    "            current_chunk.append(word)\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# does web scraping and text searching to search Baidu -> get search urls -> fetch content from urls -> process text\n",
    "def baidu_search(query): # query being the context we got\n",
    "    # defines url for baidu search\n",
    "    url = \"https://www.baidu.com/s\" # baidu (DUH)\n",
    "    \n",
    "    search_query = {'wd': query} # wd stands for word/query which is what we're searching (the context we're taking in)\n",
    "    \n",
    "    headers = { # headers r necessary bc when we as humans make searches we hv these; we need this to mimic a regular web browser's request\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    } # the User-Agent is a commonly used string for web scraping to avoid detection and blocking by webs\n",
    "    \n",
    "    # sends the get request to Baidu w our search params and header to do the search for relevant webpages\n",
    "    response = requests.get(url, params=search_query, headers=headers)\n",
    "    \n",
    "    # we need to initialize a Beautiful Soup object to actually parse the HTML so we can just get the urls\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # making a list to store urls of search results\n",
    "    results = []\n",
    "    # so a webpage will be separated in a bunch of divs as you know, and each div has a class that its associated w\n",
    "    # so when we do soup.find_all('div', class_='result'), we r using the soup object to search across all 'div's for the result class\n",
    "    # and then we r doing a for each loop of all the result divs and finding the link portion of it\n",
    "    for item in soup.find_all('div', class_='result'):\n",
    "        link = item.find('a', href=True) # 'a' is a link notation\n",
    "        if link:\n",
    "            results.append(link['href']) # and if it's a link then we add it into the list w the 'href' attribute which is a url\n",
    "\n",
    "    # now we are making another list based on our results list that takes all the urls there and calls get_page on it to get the acc page\n",
    "    docs = get_page(results) # so this stores a bunch of pages\n",
    "\n",
    "    # making another list to store the acc content of the pages in\n",
    "    content = []\n",
    "    for doc in docs: # for each doc in the docs list\n",
    "        page_text = re.sub(\"\\n\\n+\", \"\\n\", doc) # this regex part is p cool, sub() obviously substitutes something w another thing\n",
    "        # and in this case we r subbing all the \\n\\n+ w j one \\n which means that anything more than 1 newline will be replaced w a newline\n",
    "        # so we can just get all the text in a braindead way like this\n",
    "        if page_text and page_text != \"问题反馈\": # checks to see if empty string or bad text\n",
    "            # ^ note that in Python, an empty string of \"\" returns False\n",
    "            page_text = cutoff(page_text)\n",
    "            content.append(page_text) # and we r adding it to the list\n",
    "        \n",
    "    i = 0\n",
    "    for ctnt in content:\n",
    "        i += 1\n",
    "        print(\"entry \", i,\": \",ctnt)\n",
    "\n",
    "    return content # and now we r just returning a list of all the text from relevant pages\n",
    "\n",
    "# this is the function that we called in baidu_search to acc get the webpages from the urls\n",
    "def get_page(urls):\n",
    "    \n",
    "    docs = [] # we r making to store the page contents\n",
    "    # similar process w headers as above\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    # a for each loop of all the urls\n",
    "    for url in urls:\n",
    "        \n",
    "        response = requests.get(url, headers=headers) # this time we r j popping into all the pages\n",
    "        \n",
    "        if response.status_code == 200: # status code 200 means that it was successful so if we were let in, then we call beautiful soup\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser') # we initiate the soup object again\n",
    "            # ^ we get the content from the response (the response being the page that we popped into)\n",
    "            # ^ then we specify we wanna use 'html.parser' to get the info\n",
    "            paragraphs = soup.find_all('p') # 'p' = praragraph so now we r making a list of all the paragraphs from the parsed html\n",
    "            # ^ the issue is what if there are text content thats important thats not marked as 'p' or what if 'p' isn't right?\n",
    "            page_text = \"\\n\".join([p.get_text() for p in paragraphs]) # and we extract all the text from the paragraphs & join them\n",
    "            chunks = split_text(page_text) # so that it doesn't keep causing errors\n",
    "            docs.extend(chunks) # and now we add the text into our list of page content\n",
    "    # so finally we can return a list w all the page contents of j all the text on the page\n",
    "    return docs\n",
    "\n",
    "# this is just a function to cut off excess words cuz there's a max that can be embedded\n",
    "def cutoff(text):\n",
    "    words = text.split() # this splits all the text into a list of individual word strings\n",
    "    truncated = \" \".join(words[:742]) # and now we r just joining the words tgt (2000 is max)\n",
    "    return truncated\n",
    "\n",
    "# key\n",
    "qianfan_ak = \"DAEEqjuvglLTgQMCXqRvqfUj\"\n",
    "qianfan_sk = \"s0AJ849GNB6440lwLWDvGuNEJNrgrbQ3\"\n",
    "\n",
    "# models\n",
    "llm = QianfanChatEndpoint(model=\"ERNIE-4.0-8K\", streaming=True, qianfan_ak=qianfan_ak, qianfan_sk=qianfan_sk, penalty_score=1)\n",
    "embed = QianfanEmbeddingsEndpoint(model=\"bge_large_zh\", endpoint=\"bge_large_zh\", qianfan_ak=qianfan_ak, qianfan_sk=qianfan_sk)\n",
    "\n",
    "# does the rag search and returns a list (of strings) with all the info from the first few pages of web links (each link's info is concatenated tgt)\n",
    "def rag_search(query):\n",
    "    return baidu_search(query) # rn we hv them as diff functions in case baidu doesn't work out\n",
    "\n",
    "def df_mk(ar1, ar2, ar3, ar4):\n",
    "    df = pd.DataFrame({ # this is the syntax for making a df which is basically a table in pandas\n",
    "        \"产品描述\": ar1, # the quotes has the title of the column\n",
    "        \"产品卖点\": ar2, # u can either make an array like [val, val] urself\n",
    "        \"最佳营销卖点\": ar3, # or u can use an array that u alr made\n",
    "        \"目标受众\": ar4 # and put it into the df like that\n",
    "    })\n",
    "        \n",
    "    return df\n",
    "\n",
    "def parse_response(llm_output): # this parses the output param that is generated by the llm to find the info we need for the df\n",
    "    \n",
    "    # split the output into lines\n",
    "    lines = llm_output.split('\\n') # split function is a python func that turns a string into a list\n",
    "    # ^ where each newline from the og where the items r separated\n",
    "    \n",
    "    # initialize placeholders\n",
    "    product_description = 'missing description' # this is for debugging in case we find something isn't generated properly\n",
    "    selling_points = 'missing description' # then we know that whatever has \"missing description\" didn't have that part\n",
    "    best_marketing_point = 'missing description'\n",
    "    target_audience = 'missing description'\n",
    "    \n",
    "    # iterate over lines and find the relevant sections\n",
    "    for line in lines: # for each loop that searches each line in the output for the content we need\n",
    "        if line.startswith(\"1. **产品描述**：\"): # so if the line starts with [this] then we store this line into the corresponding var\n",
    "            product_description = line[len(\"1. **产品描述**：\"):].strip() # but we strip the title for cleanliness\n",
    "        elif line.startswith(\"2. **产品卖点**：\"):\n",
    "            selling_points = line[len(\"2. **产品卖点**：\"):].strip()\n",
    "        elif line.startswith(\"3. **最佳营销卖点**：\"):\n",
    "            best_marketing_point = line[len(\"3. **最佳营销卖点**：\"):].strip()\n",
    "        elif line.startswith(\"4. **目标受众**：\"):\n",
    "            target_audience = line[len(\"4. **目标受众**：\"):].strip()\n",
    "    \n",
    "    return product_description, selling_points, best_marketing_point, target_audience # and we just return the variables\n",
    "\n",
    "\n",
    "def call_marketinGPT():\n",
    "\n",
    "    file_path = \"products.txt\" # this is the file u wanna open\n",
    "    # if it was in a diff path, then u would hv to do ../folder/folder/file.txt instead\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file: # 'r' means read (DUH), utf-8 encoding is standard\n",
    "        prod_descr = file.readlines() # \"with\" makes sure it's closed at the end\n",
    "    # \"as file\" basically sets what's opened into the variable \"file\"\n",
    "    # prod_descr is a list type var that stores everything that is read from the file\n",
    "    # readlines reads all the individual lines (broken apart by \\n) into the list\n",
    "    \n",
    "    prod_descr = [desc.strip() for desc in prod_descr] # prod_descr prior to this would be like [\"prod_des1\\n\", \"prod_des2\\n\"]\n",
    "    # desc is each individual line stored in prod_descr, using desc.strip for each desc gets rid of \\n at the end of each one\n",
    "    # now, prod_descr is like [\"prod_des1\", \"prod_des2\"]\n",
    "    \n",
    "    prompt1 = PromptTemplate(\n",
    "        # template is the prompt that ur using to prompt engineer the GPT\n",
    "                \n",
    "        template = \"\"\"输入一个产品名称后，生成一段简短描述，涵盖其主要卖点、特点和优势。\\n\\n\n",
    "\n",
    "        输入格式：\\n\n",
    "        [{prod}]\\n\\n\n",
    "\n",
    "        输出格式：\\n\n",
    "        [简短描述，包括卖点、特点和优势]\\n\"\"\",\n",
    "        \n",
    "        input_variables = [\"prod\"] # here ur telling the gpt that the input variables it uses will be\n",
    "        # used where {prod} is used in the template\n",
    "    )\n",
    "    \n",
    "    prompt2 = PromptTemplate(\n",
    "                        \n",
    "        template=\"\"\"作为一名零售顾问助手，你的任务是帮助用户分析他们的产品描述，\n",
    "        并提供该产品的卖点、最佳营销卖点、目标受众以及针对目标受众的营销策略。\n",
    "        请根据以下格式进行回复，并且仅根据用户提供的信息进行分析和回答：\\n\\n\n",
    "            1. **产品描述**：用户提供的产品详细信息。\\n\n",
    "            2. **产品卖点**：根据产品描述，提炼出吸引潜在消费者的关键特点。\\n\n",
    "            3. **最佳营销卖点**：从产品卖点中选择最具市场潜力的特点，并解释为何这个卖点最有吸引力。\\n\n",
    "            4. **目标受众**：根据产品卖点，确定最适合的消费群体。\\n\\n\n",
    "            \n",
    "        以下是一个示例对话：\\n\n",
    "        \n",
    "        用户：我们有一款新型的可折叠电动自行车，重量轻，电池续航长，适合城市通勤。\\n\\n\n",
    "        系统：\\n\n",
    "            1. **产品描述**：新型可折叠电动自行车，重量轻，电池续航长，适合城市通勤。\\n\n",
    "            2. **产品卖点**：轻便设计、长续航电池、便捷的城市通勤工具。\\n\n",
    "            3. **最佳营销卖点**：长续航电池，因为城市通勤用户对续航时间有较高需求，能够减少充电频率。\\n\n",
    "            4. **目标受众**：城市白领、大学生、注重环保和便捷出行的用户。\\n\\n\n",
    "            \n",
    "        请提供您的产品描述：\\n\n",
    "        \n",
    "        {prod}\\n\\n\n",
    "\n",
    "        1. **产品描述**：用户提供的产品描述\\n\n",
    "        2. **产品卖点**：提炼出的产品卖点\\n\n",
    "        3. **最佳营销卖点**：选择的最佳营销卖点及其原因\\n\n",
    "        4. **目标受众**：确定的目标消费群体\\n\n",
    "        \n",
    "        主要使用以下信息来得出答案：\\n\\n\n",
    "        \n",
    "        {context}\"\"\",\n",
    "        \n",
    "        input_variables = [\"prod\", \"context\"] # here ur telling the gpt that the input variables it uses will be\n",
    "    )\n",
    "    \n",
    "    ar1, ar2, ar3, ar4 = [], [], [], [] # here ur declaring the arrays that the df will be made w\n",
    "\n",
    "    for prod_des in prod_descr: # this is a for-each loop which ensures that each val in the list is used\n",
    "        \n",
    "        marketinGPT = prompt1 | llm | StrOutputParser()\n",
    "        \n",
    "        ctxt = marketinGPT.invoke({\"prod\": prod_des})\n",
    "        \n",
    "        # do RAG search and embed the context and search results\n",
    "        ctxt_embed = embed.embed_query(ctxt) # uses the langchain qianfan embedder\n",
    "        search_results = rag_search(ctxt) # calls the RAG search func to get the list of search results for the context we r looking for\n",
    "        \n",
    "        all_chunks = []\n",
    "        for result in search_results:\n",
    "            chunks = split_text(result, 2000)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        search_embed = embed.embed_documents(all_chunks)\n",
    "        \n",
    "        search_embed = embed.embed_documents(search_results) # then embeds the search results asw\n",
    "        \n",
    "        # calculate dot product and get closest results\n",
    "        similarity_scores = np.dot(ctxt_embed, search_embed.T) # this will get all the dot products for our searches X context\n",
    "        top_results = sorted(zip(search_results, similarity_scores), key=lambda x: x[1], reverse=True)[:4] # this sorts the searches for highest matches\n",
    "        # the zip() function combines two lists into a list of tuples made up of the results and their scores\n",
    "        # sorted() sorts the function (DUH) based on \"key=lambda x: x[1]\" which means that it sorts on the second element (the scores)\n",
    "        # reverse=True means that it's sorted in descending order rather than ascending so top scores r on the top\n",
    "        # [:4] is the slice notation for getting the first up to :\"x\" index, so this would be the first 4 items\n",
    "        rag_result = \" \".join([result[0] for result in top_results]) # this joins the first 4 items into one string to be fed into prompt\n",
    "        \n",
    "        \n",
    "        marketinGPT = prompt2 | llm | StrOutputParser() # this is the setup for the processing pipeline\n",
    "        # prompt refers to the template ur using to prompt engineer -> this is given to llm\n",
    "        # llm then takes the text input and generates a response\n",
    "        # StrOutputParser is an output parser (DUH but also an output parser takes raw output and turns it into a structured format)\n",
    "        # ^ this turns the llm's output into something Python can easily understand\n",
    "        # using the '|' operator is basically the chaining part of the processing pipeline\n",
    "        # ^ this says the output of one component should be used as the input of the next component\n",
    "        # ^ so the prompt's output is the llm's input, the llm's output is the parser's input\n",
    "        \n",
    "        try: # we using a try-except bc who knows if the GPT will output something that is always understandable\n",
    "                        \n",
    "            ans = marketinGPT.invoke({\"prod\": prod_des, \"context\": rag_result}) # marketinGPT (brilliant name) is the name of the pipeline\n",
    "            # so when we call it, we r getting an instance of it\n",
    "            # .invoke(input val) is a method that tells the model to provide a response based on the input val\n",
    "            # \"prod\" is the input variable in the prompt, prod_des is the value in the for-each loop\n",
    "            # this lets prod_des be passed in as the input of the prompt\n",
    "            # ^ same for rag_result\n",
    "            \n",
    "            parsed_response = parse_response(ans) # here we use parse_response to parse the response (DUH)\n",
    "            \n",
    "            ar1.append(parsed_response[0]) # here we r appending (adding) the answer to the arrays\n",
    "            ar2.append(parsed_response[1]) # hopefully everything is right\n",
    "            ar3.append(parsed_response[2]) # but otherwise it will all be missing descriptions\n",
    "            ar4.append(parsed_response[3]) # so this tells us if the GPT failed to generate an expected portion\n",
    "            \n",
    "        except Exception as e: # this is just the exception portion\n",
    "            print(f\"Couldn't process: {prod_des}. Error: {e}\") # we r just saying if we couldn't process any part of the inputs\n",
    "            ar1.append('could not process')\n",
    "            ar2.append('could not process')\n",
    "            ar3.append('could not process')\n",
    "            ar4.append('could not process')\n",
    "\n",
    "    return df_mk(ar1, ar2, ar3, ar4) # finally we make the df w the arrays we built\n",
    "\n",
    "# creating the df\n",
    "df = call_marketinGPT()\n",
    "\n",
    "# styling df\n",
    "styled_df = df.style.set_properties(**{ # so we're just making another df based on the generated df but w styling\n",
    "    'background-color': 'aliceblue', # the syntax for setting stuff is [what u wanna set]: [val]\n",
    "    'color': 'black',\n",
    "    'text-align': 'center',\n",
    "    'border': '2px solid lightsteelblue !important'\n",
    "}).set_table_styles([ # below, the selector means it applies to all the following, being thead, which is the table header section (thead)'s table headers (th)\n",
    "    {'selector': 'thead th', 'props': [('background-color', 'lightslategrey'), ('color', 'white'), ('border', '2px solid darkslategray !important'), ('text-align', 'center')]}\n",
    "]).hide(axis=\"index\") # ^ marked border as !important to make sure it's done bc it kept on NOT appearing??\n",
    "\n",
    "# displaying the styled df\n",
    "display(styled_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
