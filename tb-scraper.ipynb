{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: net::ERR_CONNECTION_CLOSED\n  (Session info: chrome-headless-shell=126.0.6478.116)\nStacktrace:\n0   chromedriver                        0x0000000101052a80 chromedriver + 4385408\n1   chromedriver                        0x000000010104b38c chromedriver + 4354956\n2   chromedriver                        0x0000000100c68b0c chromedriver + 281356\n3   chromedriver                        0x0000000100c61e9c chromedriver + 253596\n4   chromedriver                        0x0000000100c53a5c chromedriver + 195164\n5   chromedriver                        0x0000000100c54e18 chromedriver + 200216\n6   chromedriver                        0x0000000100c53d78 chromedriver + 195960\n7   chromedriver                        0x0000000100c5340c chromedriver + 193548\n8   chromedriver                        0x0000000100c533b8 chromedriver + 193464\n9   chromedriver                        0x0000000100c51198 chromedriver + 184728\n10  chromedriver                        0x0000000100c51b18 chromedriver + 187160\n11  chromedriver                        0x0000000100c6b0a0 chromedriver + 290976\n12  chromedriver                        0x0000000100ce4364 chromedriver + 787300\n13  chromedriver                        0x0000000100ce3d24 chromedriver + 785700\n14  chromedriver                        0x0000000100c9feec chromedriver + 507628\n15  chromedriver                        0x0000000100ca08c4 chromedriver + 510148\n16  chromedriver                        0x000000010101a43c chromedriver + 4154428\n17  chromedriver                        0x000000010101eea0 chromedriver + 4173472\n18  chromedriver                        0x0000000100fffff8 chromedriver + 4046840\n19  chromedriver                        0x000000010101f78c chromedriver + 4175756\n20  chromedriver                        0x0000000100ff2fb8 chromedriver + 3993528\n21  chromedriver                        0x000000010103d21c chromedriver + 4297244\n22  chromedriver                        0x000000010103d398 chromedriver + 4297624\n23  chromedriver                        0x000000010104af84 chromedriver + 4353924\n24  libsystem_pthread.dylib             0x00000001947582e4 _pthread_start + 136\n25  libsystem_pthread.dylib             0x00000001947530fc thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 308\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ans\n\u001b[0;32m--> 308\u001b[0m \u001b[43mbaidu_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m茉莉花茶\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 84\u001b[0m, in \u001b[0;36mbaidu_search\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     81\u001b[0m pics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m key_pages:\n\u001b[0;32m---> 84\u001b[0m     pics[page] \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pics\n",
      "Cell \u001b[0;32mIn[35], line 120\u001b[0m, in \u001b[0;36mget_page\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_page\u001b[39m(url):\n\u001b[0;32m--> 120\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    123\u001b[0m     html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:363\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Desktop/School/summer-2024/openai-env/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: unknown error: net::ERR_CONNECTION_CLOSED\n  (Session info: chrome-headless-shell=126.0.6478.116)\nStacktrace:\n0   chromedriver                        0x0000000101052a80 chromedriver + 4385408\n1   chromedriver                        0x000000010104b38c chromedriver + 4354956\n2   chromedriver                        0x0000000100c68b0c chromedriver + 281356\n3   chromedriver                        0x0000000100c61e9c chromedriver + 253596\n4   chromedriver                        0x0000000100c53a5c chromedriver + 195164\n5   chromedriver                        0x0000000100c54e18 chromedriver + 200216\n6   chromedriver                        0x0000000100c53d78 chromedriver + 195960\n7   chromedriver                        0x0000000100c5340c chromedriver + 193548\n8   chromedriver                        0x0000000100c533b8 chromedriver + 193464\n9   chromedriver                        0x0000000100c51198 chromedriver + 184728\n10  chromedriver                        0x0000000100c51b18 chromedriver + 187160\n11  chromedriver                        0x0000000100c6b0a0 chromedriver + 290976\n12  chromedriver                        0x0000000100ce4364 chromedriver + 787300\n13  chromedriver                        0x0000000100ce3d24 chromedriver + 785700\n14  chromedriver                        0x0000000100c9feec chromedriver + 507628\n15  chromedriver                        0x0000000100ca08c4 chromedriver + 510148\n16  chromedriver                        0x000000010101a43c chromedriver + 4154428\n17  chromedriver                        0x000000010101eea0 chromedriver + 4173472\n18  chromedriver                        0x0000000100fffff8 chromedriver + 4046840\n19  chromedriver                        0x000000010101f78c chromedriver + 4175756\n20  chromedriver                        0x0000000100ff2fb8 chromedriver + 3993528\n21  chromedriver                        0x000000010103d21c chromedriver + 4297244\n22  chromedriver                        0x000000010103d398 chromedriver + 4297624\n23  chromedriver                        0x000000010104af84 chromedriver + 4353924\n24  libsystem_pthread.dylib             0x00000001947582e4 _pthread_start + 136\n25  libsystem_pthread.dylib             0x00000001947530fc thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain.embeddings import QianfanEmbeddingsEndpoint\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import numpy as np \n",
    "import requests, time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "from PIL import Image\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--headless')  \n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "service = ChromeService(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "pics = {}\n",
    "\n",
    "def split_text(text):\n",
    "    words = text.split() \n",
    "    chunks = [] \n",
    "    current_chunk = []  \n",
    "\n",
    "    for word in words:  \n",
    "        \n",
    "        if len(\" \".join(current_chunk + [word])) <= 400 and word:\n",
    "            current_chunk.append(word) \n",
    "        elif current_chunk:\n",
    "\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "            current_chunk = [word]\n",
    "\n",
    "    if current_chunk:\n",
    "\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks  \n",
    "\n",
    "def baidu_search(query):  \n",
    "    \n",
    "\n",
    "    base_url = \"https://www.baidu.com/s\"\n",
    "\n",
    "    search_query = urlencode({'wd': f\"1688 {query}\"})\n",
    "\n",
    "    headers = { \n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(f\"{base_url}?{search_query}\", headers=headers)\n",
    "        \n",
    "    if response.status_code == 200:\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        key_pages = []\n",
    "        \n",
    "        for item in soup.find_all('div', class_='result'):\n",
    "            \n",
    "            link = item.find('a', href=True) \n",
    "            \n",
    "            if link and 'href' in link.attrs:\n",
    "                \n",
    "                href = link['href']\n",
    "                url = get_acc_url(href)\n",
    "                key_pages.append(url)\n",
    "                \n",
    "                if len(key_pages) > 3:\n",
    "                    break\n",
    "            \n",
    "    else:\n",
    "        print(\"couldn't perform search\")\n",
    "    \n",
    "    for page in key_pages:\n",
    "        \n",
    "        if page[:6] == \"https:\":\n",
    "            pics[page] = get_page(page)\n",
    "        else:\n",
    "            pics[page] = get_page(\"https:\"+page[2:])\n",
    "\n",
    "    return pics\n",
    "\n",
    "\n",
    "def get_acc_url(url):\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = session.get(\n",
    "            url, headers=headers, allow_redirects=True)\n",
    "\n",
    "        if response.history:\n",
    "            actual_url = response.url\n",
    "        else:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            meta_refresh = soup.find('meta', attrs={'http-equiv': 'refresh'})\n",
    "            if meta_refresh:\n",
    "                content = meta_refresh.get('content')\n",
    "                url_start = content.find('url=') + 4\n",
    "                actual_url = content[url_start:]\n",
    "            else:\n",
    "                actual_url = url\n",
    "\n",
    "        return actual_url\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_page(url):\n",
    "        \n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    " \n",
    "    html = driver.page_source\n",
    "        \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    unique_links = {}\n",
    "        \n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        link = a_tag['href']\n",
    "        if link not in unique_links and prod_link(link):\n",
    "            unique_links[link] = True\n",
    "            \n",
    "    links = list(unique_links.keys())\n",
    "    \n",
    "    pngs = []\n",
    "    \n",
    "    for page in links:\n",
    "        process_pics(page)\n",
    "        \n",
    "    return pngs\n",
    "\n",
    "def process_pics(url):\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    screenshot = driver.save_screenshot('screenshot.png')\n",
    "    image = Image.open('screenshot.png')\n",
    "    image.save('output.png')\n",
    "    \n",
    "    return screenshot\n",
    "\n",
    "def prod_link(url):\n",
    "    if 'dj.1688.com/ci_bb?a=' in url:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# key\n",
    "qianfan_ak = \"DAEEqjuvglLTgQMCXqRvqfUj\"\n",
    "qianfan_sk = \"s0AJ849GNB6440lwLWDvGuNEJNrgrbQ3\"\n",
    "\n",
    "# models\n",
    "llm = QianfanChatEndpoint(model=\"ERNIE-4.0-8K\", streaming=True,\n",
    "                          qianfan_ak=qianfan_ak, qianfan_sk=qianfan_sk, penalty_score=1)\n",
    "embed = QianfanEmbeddingsEndpoint(\n",
    "    model=\"bge_large_zh\", endpoint=\"bge_large_zh\", qianfan_ak=qianfan_ak, qianfan_sk=qianfan_sk)\n",
    "\n",
    "\n",
    "def rag_search(query):\n",
    "    return baidu_search(query)\n",
    "\n",
    "\n",
    "def verify_info(ctxt, q):\n",
    "\n",
    "    # ask the LLM if the stuff we've collected is sufficient\n",
    "    prompt1 = PromptTemplate(\n",
    "\n",
    "        template=\"\"\"根据以下上下文：{context}，\n",
    "        评估这些信息是否足以回应以下内容：{query}，'yes' 或 'no'。\n",
    "        请深入分析，并给出是或否的回答。\"\"\",\n",
    "\n",
    "        input_variables=[\"context\", \"query\"]\n",
    "    )\n",
    "\n",
    "    marketinGPT = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "    ans = marketinGPT.invoke({\"context\": ctxt, \"query\": q})\n",
    "\n",
    "    if ans == 'yes':\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def call_marketinGPT():\n",
    "\n",
    "    file_path = \"products.txt\"  \n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        prod_descr = file.readlines()  \n",
    "    \n",
    "\n",
    "    prod_descr = [desc.strip() for desc in prod_descr]\n",
    "    \n",
    "    # ask the LLM if the stuff we've collected is sufficient\n",
    "    prompt1 = PromptTemplate(\n",
    "\n",
    "        template=\"\"\"根据以下上下文：{context}，\n",
    "        评估这些信息是否足以回应以下内容：{query}。\n",
    "        请深入分析，并给出是或否的回答。\"\"\",\n",
    "\n",
    "        input_variables=[\"context\", \"query\"]\n",
    "    )\n",
    "\n",
    "    prompt2 = PromptTemplate(\n",
    "\n",
    "        template=\"\"\"作为一名零售顾问助手，你的任务是帮助用户分析他们的产品描述，\n",
    "        并提供该产品的卖点、最佳营销卖点、目标受众以及针对目标受众的营销策略。\n",
    "        请根据以下格式进行回复，并且仅根据用户提供的信息进行分析和回答：\\n\\n\n",
    "            1. 产品描述：用户提供的产品详细信息。\\n\n",
    "            2. 产品卖点：根据产品描述，提炼出吸引潜在消费者的关键特点。\\n\n",
    "            3. 最佳营销卖点：从产品卖点中选择最具市场潜力的特点，并解释为何这个卖点最有吸引力。\\n\n",
    "            4. 目标受众：根据产品卖点，确定最适合的消费群体。\\n\\n\n",
    "            \n",
    "        以下是一个示例对话：\\n\n",
    "        \n",
    "        用户：我们有一款新型的可折叠电动自行车，重量轻，电池续航长，适合城市通勤。\\n\\n\n",
    "        系统：\\n\n",
    "            1. 产品描述：新型可折叠电动自行车，重量轻，电池续航长，适合城市通勤。\\n\n",
    "            2. 产品卖点：轻便设计、长续航电池、便捷的城市通勤工具。\\n\n",
    "            3. 最佳营销卖点：长续航电池，因为城市通勤用户对续航时间有较高需求，能够减少充电频率。\\n\n",
    "            4. 目标受众：城市白领、大学生、注重环保和便捷出行的用户。\\n\\n\n",
    "            \n",
    "        请提供您的产品描述：\\n\n",
    "        \n",
    "        {prod}\\n\\n\n",
    "\n",
    "        1. 产品描述：用户提供的产品描述\\n\n",
    "        2. 产品卖点：提炼出的产品卖点\\n\n",
    "        3. 最佳营销卖点：选择的最佳营销卖点及其原因\\n\n",
    "        4. 目标受众：确定的目标消费群体\\n\n",
    "        \n",
    "        请注意，产品卖点部分应当是一个完整的句子，不要使用任何形式的项目符号或列表，以免造成文本格式的混乱。\\n\n",
    "\n",
    "        主要使用以下信息来得出答案。即使以下信息提到其他具体产品，也要专注于该产品的优点，不提及其他产品的名称。：\\n\\n\n",
    "        \n",
    "        {context}\"\"\",\n",
    "\n",
    "        input_variables=[\"prod\", \"context\"]\n",
    "    )\n",
    "\n",
    "    for prod_des in prod_descr:  \n",
    "        \n",
    "        marketinGPT = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "        ctxt = marketinGPT.invoke({\"prod\": prod_des})\n",
    "\n",
    "        ctxt_embed = embed.embed_query(ctxt)\n",
    "\n",
    "        search_results = rag_search(ctxt)\n",
    "\n",
    "        all_chunks = []  # makes a list to store the chunks\n",
    "        for result in search_results:  # for every single result we get\n",
    "\n",
    "            chunks = split_text(result)\n",
    "\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "\n",
    "        search_embed = []  # makes a list for the embeddings bc the embedding model has a max num of tokens that is exceeded by big chunks\n",
    "        i1 = 0  # initializes two counters\n",
    "        i2 = 0\n",
    "        while i2 < len(all_chunks) - 1:  # while loop for until all chunks r processed\n",
    "            i2 += 1\n",
    "\n",
    "            search_embed.extend(embed.embed_documents(all_chunks[i1:i2]))\n",
    "            i1 += 1\n",
    "\n",
    "\n",
    "        search_embed = np.array(search_embed)\n",
    "\n",
    "        similarity_scores = np.dot(ctxt_embed, search_embed.T)\n",
    "        filtered_results = [(result, score) for result, score in zip(\n",
    "            search_results, similarity_scores) if score > 0.5]\n",
    "\n",
    "        max_ctxt = 3  # sets the max num of ctxt to 3 to not overload the model\n",
    "        if len(filtered_results) < 3:  # if there r less than 3 pieces of usable context\n",
    "            max_ctxt = len(filtered_results)  # then we use all of them\n",
    "        top_results = sorted(filtered_results, key=lambda x: x[1], reverse=True)[\n",
    "            :max_ctxt]  # this sorts the searches for highest matches\n",
    "       \n",
    "        \n",
    "        rag_results = \" \".join([result[0] for result in top_results])\n",
    "\n",
    "        marketinGPT = prompt2 | llm | StrOutputParser()\n",
    "        \n",
    "\n",
    "        try:  \n",
    "            ans = marketinGPT.invoke(\n",
    "                {\"prod\": prod_des, \"context\": rag_results})\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    return ans\n",
    "\n",
    "baidu_search(\"茉莉花茶\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
